\documentclass{report}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{setspace} 
\usepackage[ampersand]{easylist}


\usepackage{color}
\usepackage{listings}
\lstset{ %
language=C++,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}

%\singlespacing 
%\onehalfspacing 
%\doublespacing 
%\setstretch{1.1}
\renewcommand{\baselinestretch}{2}
\author{Nicolas Tollenaere}
\title{Rapport de stage de fin d'études : Évaluation et prédiction du partage des ressources mémoires
dans un programme multitâches}
\begin{document}
\maketitle
\tableofcontents




\chapter{Introduction}
Ce document rapporte mon activité durant le stage de fin d'études que j'ai 
realisé a l'Inria, l'Institut National de Recherche en Informatique et
Automatique, au sein de l'équipe CORSE (Compilation and Optimisation in Runtime).
J'ai donc eu affaires naturellement à des problématiques de recherche mais aussi à
des problématiques d'ingénierie tout au long de ces six mois. Plus précisément, mes travaux ont été
liées à des problématiques de programmation concurrente et multithreadés. Il s'est agi au départ d'évaluer
le comportement et le partage des ressources sur des architectures dites NUMA, Non-Uniform Memory Access. 
La problématique a ensuite évolué, pour des raisons que l'on verra, vers des questions de partage de cache
entre différents threads.
En l'état, mon travail a consisté en la réalisation d'un benchmark permettant d'évaluer les performances
d'une architecture lorsque qu'un ou plusieurs threads sont limités et se disputent des ressources mémoire.
On expliquera ici les difficultés qui ont été rencontrées et comment ces difficultés ont pu être dépassées.
La problématique a ensuite été d'essayer de parvenir à proposer des algorithmes pertinents pour réaliser 
des prédictions quant aux comportements des threads en concurrence. 
Plusieurs pistes et modèles ont été explorés, certains résistant mieux que d'autres aux vérifications 
expérimentales.
Après avoir rappelé le contexte du stage, le statut de l'INRIA, les thèmes de travail de l'équipe CORSE où
j'ai été intégré, nous allons voir en détail toutes les étapes qui ont été rencontrées au cours de ce stage. 
Nous remettrons ensuite en perspective ces travaux et l'intérêt qu'ils peuvent représenter dans les 
domaines traités par l'équipe.

\chapter{Le contexte}
\section{L'entreprise}

Inria est l'Institut National de Recherche en Informatique et Automatique. Créé en 1967, il a le statut
d'établissement à caractère scientifique et technologique. Sa mission est de coordonner la recherche dans 
les domaines liés à l'informatique au niveau national. Il promeut, comme on peut le trouver sur le site officiel,
l'excellence scientifique au service du transfert technologique et de la société.
\\ Depuis sa création, Inria a développé de nombreux projets scientifques qui font jusqu'à aujourd'hui 
sa réputation.  On peut citer, parmi les projets les plus connus , ses travaux sur les développements 
d'internet, les langages de programmations Caml, Caml Light et Ocaml , le logiciel de calcul scientifique 
Scilab, l'assistant de preuve Coq ou la bibliothèque de calcul flottant en haute précision GNU MPFR.
\\ Inria emploie aujourd'hui 2700 collaborateurs, issus des structures françaises mais aussi des universités
du monde entier. De par ses nombreux partenariats et son rayonnement dans la sphère scientifique de 
l'informatique, il est un acteur majeur de la recherche au niveau mondial.
Inria est également reconnu pour son implication active dans la promotion du logiciel libre. Ainsi, les
prohets Ocaml, Coq, Scilab ou GNU MPFR sont tous déposés sous licence libre, soit directement sous la 
licence GNU GPL, soit sous la licence compatible créée par Inria en collaboration avec le CEA et le CNRS
 CeCILL (CEA CNRS Inria Logiciel Libre). La bibliothèque MPFR (Multiple Precision Floating Point Reliably)
 est d'ailleurs utilisée dans le célèbre compilateur GNU GCC (GNU Compiler Collection).
\\ Les effectifs de l'Inria sont organisées en de nombreuses équipes constituées en moyenne d'une vingtaine
 de personnes (membres permanents ou non-permanents), accueillant à l'occasion (ou même de manière continue)
des chercheurs dépendants d'autres institutions. Les équipes sont à leur tour réparties sur huit centres 
autonomes dispersés sur tout le territoire métropolitain. C'est dans l'une de ces équipes que j'ai réalisé
mon stage de fin d'études.

\section{L'équipe}

Au sein de l'équipe Inria Rhones-Alpes, sur un site détaché dans les locaux de Minatech 
et du CEA de Grenoble, je suis intégré au sein de l'équipe CORSE (Compiler Optimization 
and Runtime SystEm). Créée il y a cinq ans, cette équipe est dirigée par Fabrice RASTELLO.
Comme son nom l'indique, elle s'occupe de problèmes liés à l'interaction entre la compilation
et l'exécution d'un programme, ainsi que d'autres problématiques connexes comme le déboguage.
Elle rassemble aussi bien des personnes spécialistes de la compilation que des spécialistes 
du runtime (donc de l'exécution).
\\ La conception et les perspectives des architectures matérielles ont beaucoup évolué dans les 
quinze dernières années.  Des tout débuts de l'informatique jusqu'au début des années 2000 
environ, les différents processeurs et composants ont eu tendance à converger et à se faire de
plus en plus puissantes (on parle ici de fréquence des processeurs et de nombres d'instructions 
par seconde). On imaginait généralement que cette évolution se poursuivrait et que conséquemment, 
un même programme pourrait être exécuté plus rapidement dès lors qu'on le ferait tourner sur une 
nouvelle architecture. Les années 2000 ont mis fin à cette croyance lorsque des problématiques 
telles que la consommation d'électricité ainsi que le dégagement d'énergie et de chaleur qui 
s'ensuivait et qui posait de gros problèmes quant au refroidissement du matériel sont apparus. Il 
était devenu impossible d'augmenter la puissance de calcul brute des processeurs sans détériorer 
très considérablement leur consommation d'énergie, mais aussi probablement leur durée de vie au vu du 
problème de refroidissement. Sur un tout autre plan, s'est posée la question de ce que l'on a 
appelé le Memory Gap.  Au début de l'informatique, les architectures avaient généralement une capacité
de calcul inférieure à leur capacité à rapatrier des données depuis la mémoire. Ainsi, la puissance de
calcul d'un programme se trouvait très souvent limitante par rapport à sa capacité de chargement. 
L'évolution de la technologie s'est faite de telle manière que le rapport se trouve aujourd'hui 
être inversé, c'est-à-dire que réaliser une opération simple sur une ou plusieurs données (une addition
ou une multiplication par exemple) est beaucoup plus rapide que de rapatrier cette ou ces données depuis
la mémoire principale. Dès lors, augmenter encore cette puissance de calcul peut se trouver tout à 
fait inutile dans de nombreux cas dans la mesure où bien des programmes passeront plus de temps à 
attendre l'arrivée des données qu'à réellement effectuer des calculs dessus. 
\\ Ces considérations ont rendu déraisonnable l'idée de continuer à augmenter indéfiniment la fréquence des 
processeurs. Il s'ensuivit que les architectures recommencèrent à se diversifier pour pouvoir continuer à 
offrir des performances accrues. On observe ainsi une recrudescence de machines très spécialisées, par 
exemple dans le traitement d'image (les cartes graphiques) ou dans la cryptographie. Par ailleurs, les 
architectures dites parallèles ainsi que distribuées connurent et connaissent toujours un développement 
significatif.
\\ Dans ce contexte, les problématiques de compilation et d'exécution deviennent cruciales, car un même 
programme sémantique doit être adapté de manière efficace à des cibles très diverses. Il est parfois 
très difficile, par exemple, d'exécuter de manière efficace sur une machine parallèle un programme
prévu à l'origine pour une exécution séquentielle (il est même souvent nécessaire de repenser 
entièrement le programme). Dans l'idéal, il faudrait que le programmeur n'ait en aucun cas à gérer des 
questions spécifiques à chaque architecture, à la fois dans un souci de simplicité et de portabilité.
Il serait donc souhaitable que cette charge soit entièrement dévolue au compilateur, qui lui est 
capable de cibler spécifiquement un environnement d'exécution. En pratique, tout ceci est généralement 
assez compliqué. Le travail de l'équipe se situe ici. Les membres cherchent à fournir des outils
permettant de tirer le meilleur parti d'architectures très différentes à partir d'un même code source, 
selon des métriques qui peuvent également varier avec la situation. Il s'agit généralement d'améliorer 
la performance en temps, en espace mémoire et/ou en énergie.
\\ Pour cela, l'équipe développe, pour une part, des stratégies dynamiques où les décisions sont prises au
moment de l'exécution réelle du programme sur une machine. Ces décisions peuvent concerner l'équilibrage
des charges, la répartition des tâches sur différents processeurs, etc.
\\Pour une autre part, l'équipe développe aussi des stratégies statiques, dans lesquelles les décisions
sont prises au moment de la compilation du programme. De nombreuses optimisations peuvent être réalisées
à ce moment-là grâce à une analyse du code (par exemple l'élimination de code mort, l'inlining de fonction
ou l'élimination des variables intermédiaires).
\\Mais surtout, l'équipe cherche autant que possible à développer des tactiques améliorant l'interface
entre les deux. D'un côté, le système runtime (dynamique) dispose d'informations précieuses sur 
l'environnement dans lequel le programme est exécuté. Par contre, il n'a qu'une vision partielle et 
immédiate de ce programme, puisqu'il ne peut en réaliser une analyse globale. D'un autre côté, le 
compilateur, s'il n'a pas d'informations sur le contexte précis d'exécution, réalise néanmoins une
analyse globale du code. On peut donc imaginer que le compilateur pourrait passer des informations 
utiles pour guider le runtime dans sa prise de décision.
\\Les domaines d'applications spécifiques de ces travaux sont nombreux mais ont trait essentiellement
au calcul scientifique. Des programmes liés à la physique des matériaux ou à la propagation d'ondes
ont directement tiré profit des travaux de l'équipe. Il arrive également que des partenariats se
fassent avec des fabricants de processeurs ou d'autres composants afin de réaliser un travail spécifique
d'optimisation sur leurs machines.
\chapter{Première partie du stage : OpenMP et Numa}

Ma mission était initialement liée à la technologie OpenMP. OpenMP est une interface de programmation
disponible pour les langages C, C++ et Fortran qui permet de faciliter grandement l'écriture de programmes
parallèles. Comme nous allons le voir, elle fait intervenir à la fois des techniques de compilation et des
techniques dynamique, ce qui la place au coeur des thèmes de l'équipe. L'idée était d'étudier le 
d'applications utilisant OpenMP sur des architectures particulières. Les machines ciblées étaient de type
Non Uniform Memory Architecture. Cela signifie que si chaque processeur a accès à toute la mémoire de la
machine, il a néanmoins un accès privilégié à une certaine portion de cette mémoire (on parle de noeud) et
met donc plus de temps à accéder à une donnée stockée dans un autre noeud. Il peut donc y avoir des cas où
des threads se trouvent obligés d'accéder à une mémoire distante, ce qui aurait peut-être pu être évité. 
Nous allons donc maintenant :
\begin{easylist}[checklist]
   & expliquer plus précisément le fonctionnement d'OpenMP 
   & détailler la raison d'être et les enjeux des architectures NUMA
   & montrer l'avancée dans le stage, les solutions déjà existantes et la démarche
   & montrer ce qui nous a poussé à faire évoluer le sujet de travail 
  \end{easylist}
  \section{OpenMP : fonctionnement}
OpenMP signifie Open Multi-Processing. Il s'agit, comme nous l'avons précisé précédemment, d'une interface 
de programmation pour les langages C, C++ et Fortran. OpenMP est géré par un consortium, the OpenMP 
Architecture Review Board. Plus précisément, ce consortium produit une spécification d'OpenMP, donc une 
liste des comportements et des options auquels un utilisateur peut s'attendre en utilisant OpenMP.
\\Il existe ensuite différentes implémentations, qui pourront remplir en totalité ou partiellement la 
dernière version ou une version plus ancienne de la spécification OpenMP. Chaque entreprise, groupe 
ou entité qui voudrait mettre à disposition sa version d'OpenMP a, même s'il compte respecter à la lettre
la spécification, une marge de manoeuvre très importante quant à la ``machinerie interne'' de 
l'application. Certains pourront décider notamment d'optimiser leur implémentation pour un type 
d'architecture particulière, et pourquoi pas de fournir des outils supplémentaires au programmeur. Il 
est d'ailleurs arrivé que certains de ces outils se retrouvent ensuite dans une version ultérieure 
de la spécification.
\\Nous allons d'abord évoquer OpenMP d'un point de vue utilisateur, afin de donner une meilleure idée 
de ce qu'il peut apporter. Nous rentrerons ensuite dans les détails techniques de l'implémentation, 
ce qui permettra de montrer en quoi cette technique fait intervenir une interaction entre la compilation
et l'exécution.

\subsection{Utilisation}

L'intérêt d'OpenMP est de faciliter l'écriture de programmes parallèles. Plus que cela, il s'agit aussi 
de rendre ces programmes plus portables en permettant au codeur de s'abstraire de paramètres comme le 
système d'exploitation de la machine cible (bien qu'il existe des librairies standards de multithreading, 
de nombreuses fonctionnalités avancées comme l'association d'une tâche à un cpu particulier restent encore
spécifiques à l'OS), ou le choix du nombre de threads approprié, qui dépend de l'architecture.
\\L'idée générale est de préciser les régions du code que l'on souhaite exécuter en parallèle. En C et 
C++, cela se fait au moyen de pragmas. Il s'agit de directives adressées au compilateur, sur des lignes qui 
commencent par la chaîne \#pragma omp. Une région signalé par un \#pragma omp parallel sera 
exécutée autant de fois qu'il y aura eu de threads créés par le runtime. Ce nombre correspond généralement 
au nombre de coeurs de la machine sur laquelle le programme est exécuté. Ainsi, dans le programme 
ci-dessous, Hello World sera affiché quatre fois si la machine possède quatre coeurs.
\begin{lstlisting}
#include <stdio.h>
#include <omp.h>

int main(void)
{
    #pragma omp parallel
    printf("Hello, world.\n");
    return 0;
}


\end{lstlisting}

Il faut simplement ajouter une option de compilation (fopenmp pour gcc et clang) pour compiler
correctement le programme.
De nombreuses autres possibilités sont offertes dans la mesure où les programmes concurrents ont
généralement besoin de synchronisation. Ainsi OpenMP fournit-il notamment des barrières (\#pragma
omp barrier) qui permettent d'imposer aux threads un point de synchronisation. Chaque thread se voit 
attribué un identifiant, ce qui permet de différencier les tâches selon les threads. Il est aussi
possible de faire en sorte qu'une portion du programme ne soit accomplie que par le ``master''
(\#pragma omp master - le master est simplement le thread qui possède l'id 0), d'exiger que cette
portion ne soit accédée que par un thread à la fois, voire par un seul thread, peu importe lequel.
Ainsi, considérons l'exemple suivant : 
\begin{lstlisting}
#include <stdio.h>
#include <omp.h>

int main(void)
{
#pragma omp parallel
  {
    int tid = omp_get_thread_num();
    printf("Hello, world from thread \%d\n", tid);
#pragma omp master
    printf("I'm the master ! \n" );
  }
}
\end{lstlisting}
Une sortie typique de ce programme serait, par exemple :
\\Hello, world from thread 2
\\Hello, world from thread 0
\\Hello, world from thread 1
\\Hello, world from thread 3
\\ I'm the master ! 

Mais il est aussi possible d'avoir :
\\Hello, world from thread 2
\\Hello, world from thread 0
\\ I'm the master ! 
\\Hello, world from thread 1
\\Hello, world from thread 3

 L'instruction barrier nous permet d'éviter cela.
\begin{lstlisting}
#include <stdio.h>
#include <omp.h>

int main(void)
{
#pragma omp parallel
  {
    int tid = omp_get_thread_num();
    printf("Hello, world from thread \%d\n", tid);
#pragma omp barrier
#pragma omp master
    printf("I'm the master ! \n" );
  }
}


\end{lstlisting}

De la sorte, on a contraint le programme à afficher I'm the master seulement une fois que chacun 
des threads aura affiché son Hello.

La politique de partage de ressources entre les threads est également un élément clé de la programmation
concurrente. Conséquemment, OpenMP offre la possibilité de déclarer une variable ``shared'' ou ``private''.
Dans le premier cas, elle sera visible et modifiable par tous les threads, et son accès nécessitera sans
doute une synchronisation. Dans le second cas, chaque thread aura une copie de la variable invisible aux 
autres. Il existe des variations que nous n'allons pas discuter.
\\Il n'est pas utile de rentrer ici plus longuement dans les détails. Ajoutons simplement que la
norme a bien entendu énormément évolué au cours du temps pour répondre à des besoins de plus en plus
variés. Ainsi, il est désormais possible de réaliser des parallélisations vectorielles. Des possibilités
ont également été ajoutées pour pouvoir changer à loisir le nombre de threads engagés ainsi que le coeur 
sur lequel ils s'exécuteront. Voyons maintenant comment se déroule concrètement la compilation et
l'exécution d'un programme OpenMP.

\subsection{Fonctionnement}

La spécification OpenMP jouit de nombreuses implémentations. Deux sont particulièrement notables : celle
qui est fournie avec GCC (Gnu Compiler Collection) et celle développée Intel, utilisée en particulier
par Clang. Cependant, il en existe d'autres, dont certaines sont même développées à l'Inria, comme Kaapi
ou StarPU.
\\Il convient maintenant de mieux préciser le fonctionnement d'une implémentation OpenMP. Celui-ci est
divisé en deux parties : une part du travail est réalisée à la compilation. En se basant sur les pragmas
écrites par le programmeur, le compilateur va isoler dans le code les portions destinées à être exécutées
en parallèle, et effectuer quelques autres transformations. 
\\ La deuxième partie du travail sera effectuée directement à l'exécution par un système runtime. À ce 
moment seront déterminés des paramètres tels que le nombre de threads. Il faut bien comprendre que si les
deux parties sont bien distinctes, elles sont néanmoins fortement interdépendantes : en règle générale,
un programme compilé avec un compilateur donné ne pourra fonctionner qu'avec le runtime correspondant,
même si cette limitation est contournable. Cela explique pourquoi les deux implémentations majeures ont
été développés par des entités fortement impliquées dans le domaine de la  compilation, à 
savoir GCC et Intel. 
\\Voyons un peu plus en détails chacun des deux points, en commençant par la compilation. Comme dit plus
haut, l'idée ici est d'isoler le code à paralléliser du reste et d'introduire au bon endroit des appels
aux fonctionnalités appropriées du runtime. Pour cela, le compilateur réalise une opération
d'``outilining''. Comme le nom l'indique, il s'agit de l'exact inverse de l'inlining. Là où l'inlining
prend le code d'une fonction pour l'injecter à l'endroit où la fonction en question est appelée, 
l'outilining sélectionne une portion du code et la remplace par un appel à une fonction comprenant le
code correspondant. On supprime une indirection dans le premier cas là où on en rajoute une dans le 
second. Ainsi, le code suivant :
\begin{lstlisting}
#include <stdio.h>
#include <omp.h>

int main(void)
{
    #pragma omp parallel
    printf("Hello, world.\n");
    return 0;
}


\end{lstlisting}

pourra être remplacé par celui-ci :
\begin{lstlisting}
#include <stdio.h>
#include <omp.h>

void omp_fun()
{
    printf("Hello, world.\n");
}

int main(void)
{
    omp_fun();
    return 0;
}



\end{lstlisting}
Le nom de la fonction choisi ici est un exemple : il dépend du compilateur utilisé. Cette opération est
de toute façon transparente pour l'utilisateur. 
\\L'outlining seul ne suffit pas : l'objectif concret est d'indiquer au runtime sur quelle partie du code
il est censé travailler. Pour cela, la fonction qui a été outlinée va en fait être passée en paramètre
(sous forme d'un pointeur) à une fonction de la bibliothèque du runtime. Pour le runtime d'Intel, la 
fonction d'entrée s'appelle kmpc\_fork\_call. On aura donc, schématiquement : 

\begin{lstlisting}
#include <stdio.h>
#include <omp.h>

void omp_outlined_fun()
{
    printf("Hello, world.\n");
}

int main(void)
{
    kmpc_fork_call((void (void)*)omp_outlined_fun);
    return 0;
}

\end{lstlisting}

D'autres arguments sont également passés à la fonction. Pour ce qui est des variables, un pointeur est
passé en argument de la fonction outlinée si elle est partagée, ou la variable est placée directement
dans la pile (elle est donc déclarée au début du corps de la fonction) quand elle est privée.
\\Tout marche grossièrement de cette manière : les différentes directives passées au travers des pragmas
sont associés à des fonctions du runtime auxquelles des appels sont faits dans le programme produit par
le compilateur. 
\\Quant au runtime, il s'agit en fait simplement d'une bibliothèque dynamique. Cette bibliothèque 
contient toutes les fonctions évoquées au-dessus. Comme la bibliothèque est dynamique, le linkage est fait
au moment de l'exécution plutôt qu'au moment de la compilation, ce qui autorise un programme à utiliser
des versions plus récentes que celles qui existaient lorsqu'il a été compilé (pour peu qu'on ait assuré
la rétrocompatibilité de la bibliothèque). Il faut cependant que le compilateur connaissent la signature
(nom et type des arguments) des fonctions utilisées. C'est la raison de la dépendance runtime/compilateur
déjà évoquée. Comme on l'a dit, cette limitation peut néanmoins être contournée. Ainsi, le support 
d'exécution d'Intel peut exécuter des programmes compilés avec gcc. Pour cela, il inclue dans son code
des redirections des fonctions du runtime de gcc vers les fonctions de son propre runtime. Néanmoins,
ceci n'est pas généralisable.

\end{document}
